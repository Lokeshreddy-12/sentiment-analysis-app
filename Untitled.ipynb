{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fa90f-9096-4f38-a9b6-c218eee42b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: textblob in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (0.19.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (4.49.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: dash in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (3.0.0)\n",
      "Requirement already satisfied: shap in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (0.47.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (1.9.4)\n",
      "Requirement already satisfied: xgboost in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (3.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (3.3.2)\n",
      "Requirement already satisfied: ipython in c:\\programdata\\anaconda3\\lib\\site-packages (8.27.0)\n",
      "Requirement already satisfied: flask in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (2.14.1)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: Werkzeug<3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from dash) (7.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dash) (4.11.0)\n",
      "Requirement already satisfied: retrying in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from dash) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from dash) (75.1.0)\n",
      "Requirement already satisfied: stringcase>=1.2.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from dash) (1.2.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython) (0.4.6)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.43.0)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mallu\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata->dash) (3.17.0)\n",
      "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk textblob transformers matplotlib seaborn plotly dash shap wordcloud xgboost torch tensorflow vaderSentiment ipython flask accelerate emoji joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a526ad-6060-4c02-8450-48096c2ae5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mallu\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mallu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mallu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mallu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mallu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from IMDB-Dataset.csv...\n",
      "Dataset Shape: (50000, 3)\n",
      "                                                text  label  \\\n",
      "0  One of the other reviewers has mentioned that ...      1   \n",
      "1  A wonderful little production. <br /><br />The...      1   \n",
      "2  I thought this was a wonderful way to spend ti...      1   \n",
      "3  Basically there's a family where a little boy ...      0   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...      1   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  one reviewer mentioned watching 1 oz episode y...  \n",
      "1  wonderful little production br br filming tech...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically there family little boy jake think t...  \n",
      "4  petter matteis love time money visually stunni...  \n",
      "TextBlob Sentiment Analysis:\n",
      "                                        cleaned_text sentiment\n",
      "0  one reviewer mentioned watching 1 oz episode y...  Positive\n",
      "1  wonderful little production br br filming tech...  Positive\n",
      "2  thought wonderful way spend time hot summer we...  Positive\n",
      "3  basically there family little boy jake think t...  Positive\n",
      "4  petter matteis love time money visually stunni...  Positive\n",
      "\n",
      "VADER Sentiment Analysis (first 10 rows):\n",
      "                                        cleaned_text sentiment  \\\n",
      "0  one reviewer mentioned watching 1 oz episode y...  Positive   \n",
      "1  wonderful little production br br filming tech...  Positive   \n",
      "2  thought wonderful way spend time hot summer we...  Positive   \n",
      "3  basically there family little boy jake think t...  Positive   \n",
      "4  petter matteis love time money visually stunni...  Positive   \n",
      "\n",
      "                                            emotions  \n",
      "0  {'positive': 0.09, 'negative': 0.307, 'neutral...  \n",
      "1  {'positive': 0.267, 'negative': 0.075, 'neutra...  \n",
      "2  {'positive': 0.299, 'negative': 0.147, 'neutra...  \n",
      "3  {'positive': 0.125, 'negative': 0.216, 'neutra...  \n",
      "4  {'positive': 0.249, 'negative': 0.03, 'neutral...  \n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "\n",
      "Training traditional models...\n",
      "Logistic Regression Accuracy: 0.8871\n",
      "Naive Bayes Accuracy: 0.8543\n",
      "Decision Tree Accuracy: 0.7295\n",
      "Random Forest Accuracy: 0.8272\n",
      "XGBoost Accuracy: 0.8608\n",
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.5211 - loss: 0.6935 - val_accuracy: 0.5250 - val_loss: 0.6923\n",
      "Epoch 2/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 32ms/step - accuracy: 0.6210 - loss: 0.6688 - val_accuracy: 0.6520 - val_loss: 0.6446\n",
      "Epoch 3/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.6226 - loss: 0.6459 - val_accuracy: 0.6440 - val_loss: 0.6361\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "LSTM Accuracy: 0.644\n",
      "\n",
      "Training BERT model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 04:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.422123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.815\n",
      "\n",
      "Final Model Accuracies:\n",
      "Logistic Regression: 0.8871\n",
      "Naive Bayes: 0.8543\n",
      "Decision Tree: 0.7295\n",
      "Random Forest: 0.8272\n",
      "XGBoost: 0.8608\n",
      "LSTM: 0.644\n",
      "BERT: 0.815\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Saving the best model...\n",
      "Best Model: Logistic Regression with Accuracy: 0.8871\n",
      "Metrics saved to 'model_metrics.pkl'\n",
      "Files saved: ['.ipynb_checkpoints', 'app.py', 'IMDB-Dataset.csv', 'logs', 'model_accuracy_bar.png', 'model_metrics.pkl', 'results', 'sentiment_model.pkl', 'static', 'templates', 'Untitled.ipynb', 'vectorizer.pkl', 'wordcloud_negative.png', 'wordcloud_positive.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)  # Suppress Keras warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # Suppress FutureWarnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import emoji\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "label_mapping = {1: \"Positive üôÇ\", 0: \"Negative üòû\"}\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def load_and_preprocess_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        raise ValueError(\"Dataset must have 'text' and 'label' columns\")\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    df['label'] = df['label'].map({'positive': 1, 'negative': 0})\n",
    "    return df\n",
    "\n",
    "def get_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    return \"Positive\" if polarity > 0 else \"Negative\" if polarity < 0 else \"Neutral\"\n",
    "\n",
    "def get_vader_emotions(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return {\"positive\": scores['pos'], \"negative\": scores['neg'], \"neutral\": scores['neu'], \"compound\": scores['compound']}\n",
    "\n",
    "def apply_sentiment_analysis(df):\n",
    "    df['sentiment'] = df['cleaned_text'].apply(get_sentiment)\n",
    "    print(\"TextBlob Sentiment Analysis:\")\n",
    "    print(df[['cleaned_text', 'sentiment']].head())\n",
    "    df_subset = df.head(10).copy()\n",
    "    df_subset.loc[:, 'emotions'] = df_subset['cleaned_text'].apply(get_vader_emotions)\n",
    "    print(\"\\nVADER Sentiment Analysis (first 10 rows):\")\n",
    "    print(df_subset[['cleaned_text', 'sentiment', 'emotions']].head())\n",
    "    return df\n",
    "\n",
    "def train_traditional_models(X_train, X_test, y_train, y_test):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=10),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss')\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        y_prob = model.predict_proba(X_test_vec)[:, 1]  # Probabilities for ROC\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        results[name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"report\": report,\n",
    "            \"fpr\": fpr.tolist(),  # Convert to list for serialization\n",
    "            \"tpr\": tpr.tolist(),\n",
    "            \"roc_auc\": roc_auc\n",
    "        }\n",
    "        print(f\"{name} Accuracy: {accuracy}\")\n",
    "    return models, vectorizer, results\n",
    "\n",
    "def train_lstm(X_train, X_test, y_train, y_test):\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    max_length = 100\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "\n",
    "    lstm_model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128),  # Removed input_length\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = lstm_model.fit(X_train_pad[:5000], y_train_np[:5000],\n",
    "                            epochs=3, batch_size=32,\n",
    "                            validation_data=(X_test_pad[:1000], y_test_np[:1000]),\n",
    "                            verbose=1)\n",
    "\n",
    "    y_prob_lstm = lstm_model.predict(X_test_pad[:1000])\n",
    "    y_pred_lstm = (y_prob_lstm > 0.5).astype(int)\n",
    "    lstm_accuracy = accuracy_score(y_test_np[:1000], y_pred_lstm)\n",
    "    fpr, tpr, _ = roc_curve(y_test_np[:1000], y_prob_lstm)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lstm_results = {\n",
    "        \"LSTM\": {\n",
    "            \"accuracy\": lstm_accuracy,\n",
    "            \"report\": classification_report(y_test_np[:1000], y_pred_lstm, output_dict=True),\n",
    "            \"fpr\": fpr.tolist(),\n",
    "            \"tpr\": tpr.tolist(),\n",
    "            \"roc_auc\": roc_auc\n",
    "        }\n",
    "    }\n",
    "    print(f\"LSTM Accuracy: {lstm_accuracy}\")\n",
    "    return lstm_model, tokenizer, lstm_results\n",
    "\n",
    "def train_bert(X_train, X_test, y_train, y_test):\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    train_texts = X_train[:1000].tolist()\n",
    "    test_texts = X_test[:200].tolist()\n",
    "    train_labels = y_train[:1000].tolist()\n",
    "    test_labels = y_test[:200].tolist()\n",
    "\n",
    "    train_encodings = tokenizer_bert(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    test_encodings = tokenizer_bert(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    class SentimentDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "    test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\"  # Updated from evaluation_strategy\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=bert_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_prob_bert = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()[:, 1]\n",
    "    y_pred_bert = np.argmax(predictions.predictions, axis=1)\n",
    "    bert_accuracy = accuracy_score(test_labels, y_pred_bert)\n",
    "    fpr, tpr, _ = roc_curve(test_labels, y_prob_bert)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    bert_results = {\n",
    "        \"BERT\": {\n",
    "            \"accuracy\": bert_accuracy,\n",
    "            \"report\": classification_report(test_labels, y_pred_bert, output_dict=True),\n",
    "            \"fpr\": fpr.tolist(),\n",
    "            \"tpr\": tpr.tolist(),\n",
    "            \"roc_auc\": roc_auc\n",
    "        }\n",
    "    }\n",
    "    print(f\"BERT Accuracy: {bert_accuracy}\")\n",
    "    return bert_model, tokenizer_bert, bert_results\n",
    "\n",
    "def visualize_model_performance(results):\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [results[name][\"accuracy\"] for name in model_names]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=accuracies, y=model_names, hue=model_names, palette=\"viridis\", legend=False)\n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.savefig(\"model_accuracy_bar.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_wordclouds(df):\n",
    "    positive_text = ' '.join(df[df['sentiment'] == 'Positive']['cleaned_text'])\n",
    "    if positive_text.strip():\n",
    "        wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Word Cloud for Positive Sentiment\")\n",
    "        plt.savefig(\"wordcloud_positive.png\")\n",
    "        plt.close()\n",
    "\n",
    "    negative_text = ' '.join(df[df['sentiment'] == 'Negative']['cleaned_text'])\n",
    "    if negative_text.strip():\n",
    "        wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Word Cloud for Negative Sentiment\")\n",
    "        plt.savefig(\"wordcloud_negative.png\")\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    dataset_path = 'IMDB-Dataset.csv'\n",
    "    print(f\"Loading dataset from {dataset_path}...\")\n",
    "    df = load_and_preprocess_dataset(dataset_path)\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    df = apply_sentiment_analysis(df)\n",
    "\n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    X = df['cleaned_text']\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"\\nTraining traditional models...\")\n",
    "    models, vectorizer, results = train_traditional_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    lstm_model, lstm_tokenizer, lstm_results = train_lstm(X_train, X_test, y_train, y_test)\n",
    "    results.update(lstm_results)\n",
    "\n",
    "    print(\"\\nTraining BERT model...\")\n",
    "    bert_model, bert_tokenizer, bert_results = train_bert(X_train, X_test, y_train, y_test)\n",
    "    results.update(bert_results)\n",
    "\n",
    "    print(\"\\nFinal Model Accuracies:\")\n",
    "    for name in results:\n",
    "        print(f\"{name}: {results[name]['accuracy']}\")\n",
    "\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualize_model_performance(results)\n",
    "    generate_wordclouds(df)\n",
    "\n",
    "    print(\"\\nSaving the best model...\")\n",
    "    best_model_name = max(results, key=lambda k: results[k][\"accuracy\"])\n",
    "    print(f\"Best Model: {best_model_name} with Accuracy: {results[best_model_name]['accuracy']}\")\n",
    "\n",
    "    if best_model_name == \"LSTM\":\n",
    "        lstm_model.save(\"lstm_sentiment_model.h5\")\n",
    "        joblib.dump(lstm_tokenizer, \"lstm_tokenizer.pkl\")\n",
    "    elif best_model_name == \"BERT\":\n",
    "        bert_model.save_pretrained(\"bert_sentiment_model\")\n",
    "        bert_tokenizer.save_pretrained(\"bert_sentiment_model\")\n",
    "    else:\n",
    "        best_model = models[best_model_name]\n",
    "        joblib.dump(best_model, \"sentiment_model.pkl\")\n",
    "        joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "\n",
    "    joblib.dump(results, \"model_metrics.pkl\")\n",
    "    print(\"Metrics saved to 'model_metrics.pkl'\")\n",
    "    print(\"Files saved:\", os.listdir('.'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
